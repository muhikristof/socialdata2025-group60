{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining historical and new data\n",
    "\n",
    "See more at [SFPD_DataCombiner.ipynb](SFPD_DataCombiner.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the dataframe\n",
    "\n",
    "This part reorganises the SFPD data by finding reports with the same report ID (ie crimes that relate to eachother) and compiles all column values associated with that report ID into lists organised by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import json\n",
    "\n",
    "#load the data\n",
    "df = pd.read_csv('PDReports_2003_2024.csv')  # Assuming the data is in a CSV file\n",
    "\n",
    "# filter data to only include rows with non-unique values in 'Incident Number'\n",
    "df = df[df.duplicated('Incident Number', keep=False)]\n",
    "\n",
    "# remove ones with the same Time ie. has same incident number bcuz officer filed multiple reports\n",
    "df = df[~df.duplicated(subset=['Incident Number', 'Time'], keep=False)]\n",
    "\n",
    "# now remove non unique ones\n",
    "df = df[df.duplicated('Incident Number', keep=False)]\n",
    "\n",
    "# remove ones with the same Time ie. has same incident number bcuz officer filed multiple reports\n",
    "df = df[~df.duplicated(subset=['Incident Number', 'Longitude', 'Latitude'], keep=False)]\n",
    "\n",
    "# now remove non unique ones\n",
    "df = df[df.duplicated('Incident Number', keep=False)]\n",
    "\n",
    "# list of unique crime types and their counts\n",
    "# counts = df['Category'].value_counts()\n",
    "\n",
    "# sort the dataset by 'Date' and 'Time'\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d').dt.date\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='%H:%M').dt.time\n",
    "df = df.sort_values(by=['Date', 'Time'])\n",
    "df['Date'] = df['Date'].astype(str)\n",
    "df['Time'] = df['Time'].astype(str)\n",
    "\n",
    "# list of unique crime types and their counts\n",
    "# counts = df['Category'].value_counts()\n",
    "\n",
    "# apply the following to the dataframe\n",
    "# 1. get all unique 'Incident Number' values\n",
    "# 2. for each unique value, get a list of all entries with that value in 'Incident Number'\n",
    "# 3. have column for incident number, list of dates and times, locations (lat, long), incident category\n",
    "# 4. create a new dataframe with the above columns\n",
    "unique_incidents = df['Incident Number'].unique()\n",
    "incident_data = []\n",
    "\n",
    "for incident in unique_incidents:\n",
    "    incident_entries = df[df['Incident Number'] == incident]\n",
    "    dates = incident_entries['Date'].tolist()\n",
    "    times = incident_entries['Time'].tolist()\n",
    "    longitudes = incident_entries['Longitude'].tolist()\n",
    "    latitudes = incident_entries['Latitude'].tolist()\n",
    "    categories = incident_entries['Category'].tolist()\n",
    "    descriptions = incident_entries['Description'].tolist()\n",
    "    disctricts = incident_entries['District'].tolist()\n",
    "    dayOfWeeks = incident_entries['DayOfWeek'].tolist()\n",
    "    \n",
    "    # Append the data for this incident to the list\n",
    "    incident_data.append({\n",
    "        'Incident Number': incident,\n",
    "        'Dates': dates,\n",
    "        'Times': times,\n",
    "        'DayOfWeeks': dayOfWeeks,\n",
    "        'Categories': categories,\n",
    "        'Descriptions': descriptions,\n",
    "        'Longitudes': longitudes,\n",
    "        'Latitudes': latitudes,\n",
    "        'Districts': disctricts\n",
    "    })\n",
    "\n",
    "incident_data = pd.DataFrame(incident_data)\n",
    "\n",
    "# incident_data['Categories'].value_counts()\n",
    "\n",
    "# filter the dataframe to only include rows with 'Categories' containing 'Vehicle Theft' in first position and 'Vehicle Related' in second position\n",
    "carJobs = incident_data[incident_data['Categories'].apply(lambda x: 'Vehicle Theft' == x[0] and 'Vehicle Related' == x[1])]\n",
    "\n",
    "# filter out rows with missing coordinates\n",
    "carJobs = carJobs[carJobs['Longitudes'].apply(lambda x: not (np.isnan(x[0]) or np.isnan(x[1])))]  \n",
    "\n",
    "# reorganise longitudes and latitudes into a list of old and new points [[lat1, long1], [lat2, long2]] \n",
    "carJobs['pointpair'] = carJobs.apply(lambda x: [[x['Latitudes'][0], x['Longitudes'][0]],[x['Latitudes'][1], x['Longitudes'][1]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "carJobs.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinate vector from first point to second point\n",
    "def delta(pointpair):\n",
    "    return [pointpair[1][0]-pointpair[0][0], pointpair[1][1]-pointpair[0][1]]\n",
    "\n",
    "# get normalized vector from first point to second point\n",
    "def normaldelta(pointpair):\n",
    "    d = delta(pointpair)\n",
    "    dn = math.sqrt(d[0]**2 + (d[1])**2)\n",
    "    return [d[0]/dn, d[1]/dn]\n",
    "\n",
    "# get the angle of the vector from first point to second point in radians\n",
    "def getAngle(pointpair):\n",
    "    d = delta(pointpair)\n",
    "    dn = math.sqrt(d[0]**2 + (d[1])**2)\n",
    "    d = [d[0]/dn, d[1]/dn]  # Normalize the vector\n",
    "    angle = math.atan2(d[0], d[1])\n",
    "    return angle\n",
    "\n",
    "# get vector from first point to second point but cut it off to a fraction of it's length\n",
    "def cutoff(pointpair, cutoff=0.1):\n",
    "    d = delta(pointpair)\n",
    "    new_destination = [pointpair[0][0] + d[0]*cutoff, pointpair[0][1] + d[1]*cutoff]\n",
    "    return [pointpair[0],new_destination]\n",
    "\n",
    "# get vector from first point to second point but make it a fixed length\n",
    "def normalcutoff(pointpair, length=0.01):\n",
    "    d = delta(pointpair)\n",
    "    dn = math.sqrt(d[0]**2 + (d[1])**2)\n",
    "    d = [d[0]/dn, d[1]/dn]  # Normalize the vector\n",
    "    new_destination = [pointpair[0][0] + d[0]*length, pointpair[0][1] + d[1]*length]\n",
    "    return [pointpair[0],new_destination]\n",
    "\n",
    "# pointpair reorganises the latitudes and longitudes columns into one column of a nested list [[lat1, long1], [lat2, long2]]\n",
    "carJobs['pointpair'].head(20).apply(lambda x: delta(normalcutoff(x)))\n",
    "# arrow is the angle of the vector from the first point to the second point in radians\n",
    "carJobs['arrow'] = carJobs['pointpair'].apply(lambda x: getAngle(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The heatmap plots\n",
    "\n",
    "This makes a heatmap of car thefts for each district\n",
    "District gets marked by a blue line\n",
    "Heatmap shows where cars have been found that have been stolen from that district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoJSON data\n",
    "with open(\"sfpd.geojson\", \"r\") as f:\n",
    "    geo_data = json.load(f)\n",
    "\n",
    "unique_districts = carJobs['Districts'].apply(lambda x: x[0]).unique()\n",
    "\n",
    "for district in unique_districts:\n",
    "    # get only car thefts from one district\n",
    "    subset = carJobs[carJobs['Districts'].apply(lambda x: x[0] == district)]\n",
    "    filtered_geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": [f for f in geo_data[\"features\"] if f[\"properties\"][\"DISTRICT\"] == district]\n",
    "    }\n",
    "\n",
    "    # Create a map centered at an average location\n",
    "    m = folium.Map(location=[37.7749, -122.4194],tiles='Cartodb Positron', zoom_start=12)\n",
    "\n",
    "    # Add GeoJSON layer to the map\n",
    "    folium.GeoJson(filtered_geojson,name=\"GeoJSON Layer\").add_to(m)\n",
    "\n",
    "    # Create heatmap for the locations of stolen vehicles being found\n",
    "    heat_data = []\n",
    "    for index, row in subset.iterrows():\n",
    "        heat_data.append(row['pointpair'][1])\n",
    "\n",
    "    # Add heatmap layer to the map\n",
    "    HeatMap(heat_data, radius=12).add_to(m)\n",
    "\n",
    "    # Save the map\n",
    "    m.save(f'heatmaps/heatmap_{district}.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polar charts\n",
    "\n",
    "Plots about what directions cars stolen from a given district where they have been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_districts = carJobs['Districts'].apply(lambda x: x[0]).unique()\n",
    "num_districts = len(unique_districts)\n",
    "num_columns = 3\n",
    "num_rows = (num_districts + num_columns - 1) // num_columns  # Calculate rows needed\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, 5 * num_rows), constrained_layout=True)\n",
    "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "for i, district in enumerate(unique_districts):\n",
    "    ax = axes[i]\n",
    "    ax = plt.subplot(num_rows, num_columns, i + 1, polar=True)\n",
    "    ax.hist(carJobs[carJobs['Districts'].apply(lambda x: x[0]) == district]['arrow'], bins=50, alpha=0.5)\n",
    "    ax.set_title(f'Angle Distribution for {district} District')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot that shows the distance distribution (how far they have been found) of all car thefts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_distance(coord1, coord2):\n",
    "    lat1, lon1 = coord1\n",
    "    lat2, lon2 = coord2\n",
    "\n",
    "    lat_factor = 111320  # Meters per degree latitude\n",
    "    lon_factor = 111320 * math.cos(math.radians(lat1))  # Adjust for latitude\n",
    "\n",
    "    dx = (lon2 - lon1) * lon_factor\n",
    "    dy = (lat2 - lat1) * lat_factor\n",
    "\n",
    "    return math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "carJobs['Distance'] = carJobs['pointpair'].apply(lambda x: approx_distance(x[0],x[1]))\n",
    "carJobs['Distance'].describe()\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "carJobs[carJobs['Distance'] > 1]['Distance'].hist(bins=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
